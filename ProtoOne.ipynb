{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Current Problem\n",
    "\n",
    "#task split function creates a chain of tasks, but it should be a hierarchy\n",
    "\n",
    "#provide taskSolver most helpful results of task childs (sometimes context is polutet)\n",
    "\n",
    "#results are added in the wrong order in dummySolver\n",
    "\n",
    "#an return all usefull information in this text for this task( and the context of the taskpath)\n",
    "\n",
    "# minor:Check if task is solved correctly could be done with fewer tokens\n",
    "\n",
    "# https://www.reddit.com/r/AutoGPT/comments/12k4al2/found_the_winning_sauce/\n",
    "\n",
    "#IDEA Make the VirtualFileOperations so they contain a message what has been done. This could be used to create a log of the process that can be used to create the TaskResultMessage\n",
    "\n",
    "# use caching of LLM calls\n",
    "\n",
    "#make tooluse an process:\n",
    "    # what you wane do? i suggest tools?\n",
    "    # what suggested tool you wane use?\n",
    "    # heres a description how exacly to use that tool\n",
    "    # execute the tool\n",
    "\n",
    "#fundamental a workingdir with write and read access and a obligatory paginated output is needed\n",
    "#solver could have verry simple read commands like \n",
    "#READ FILENAME PAGE\n",
    "#WRITE FILENAME PAGE\n",
    "#LIST FILENAMES\n",
    "\n",
    "#TODO READ FIle with sourounding should indicate where surounding is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we gone need many types of BrainFunctions\n",
    "\n",
    "#Tasks\n",
    "    #tasks have subtasks\n",
    "    #if a task couldn't be solved, it can be retried later\n",
    "    #tasks should consist of descriptions results and parameters\n",
    "\n",
    "#taskpath \n",
    "    #Path from Objectiv Task to current Task. Creates a great context for the current task.\n",
    "\n",
    "#Atomic Tasks are tasks with no subtasks\n",
    "\n",
    "#Central control\n",
    "#    decides what to do next.\n",
    "#       Split Tast\n",
    "#       Solve Task\n",
    "#       Retry Task\n",
    "#       Improve Task Solution\n",
    "\n",
    "#TaskSplitter Task->List[Task] (simpler tasks)\n",
    "#TaskSolver Task->Result + Optional List[Task] (new tasks encountered during solving, debugging, etc.)\n",
    "#Task Duplicate Detector List[Task]->List[Task]\n",
    "#Task Merger List[Task]->Task\n",
    "#Task Selector \n",
    "#Function tath gathers all the results that can be helpfull to solve a task\n",
    "#asign solved task to other tasks that could benefit from the results\n",
    "#ask for most complex task and split it into subtasks\n",
    "\n",
    "\n",
    "#JSON Stuffer Takes result of Brainfunction and stuffs it into a JSON string\n",
    "#retries with \"doesnt look like correct JSNO jet\" until its successfull parsed into object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Task\n",
      "Now in SplitTask:Create a file summary.txt containing a summary of what FinalAGI.py does, by iterating over all the pages of FinalAGI.py.\n",
      "Splitted Tasks before JSON packing: Task 1: Summarize the long text or code snippet\n",
      "- CREATE_FILE \"summary.txt\"\n",
      "- Instruct the user to write a concise summary of the long text or code snippet, capturing the essential information needed to understand the context.\n",
      "- FINISH\n",
      "Expected result: A file named \"summary.txt\" containing a summary of the long text or code snippet.\n",
      "\n",
      "Task 2: Break down the long text or code snippet into smaller chunks\n",
      "- CREATE_FILE \"chunks.txt\"\n",
      "- Instruct the user to divide the long text or code snippet into smaller, manageable chunks that fit within the model's token limit.\n",
      "- APPEND_TO_FILE \"chunks.txt\" for each chunk created.\n",
      "- FINISH\n",
      "Expected result: A file named \"chunks.txt\" containing the smaller chunks of the long text or code snippet.\n",
      "\n",
      "Task 3: Process each chunk separately\n",
      "- READ_FILE \"chunks.txt\"\n",
      "- Instruct the user to process each chunk separately with the language model, ensuring that the context is maintained across chunks.\n",
      "- CREATE_FILE \"processed_chunks.txt\"\n",
      "- APPEND_TO_FILE \"processed_chunks.txt\" for each processed chunk.\n",
      "- FINISH\n",
      "Expected result: A file named \"processed_chunks.txt\" containing the processed chunks of the long text or code snippet.\n",
      "\n",
      "Task 4: Divide the content into smaller pages or sections\n",
      "- CREATE_FILE \"pages.txt\"\n",
      "- Instruct the user to divide the content of the long text or code snippet into smaller pages or sections.\n",
      "- APPEND_TO_FILE \"pages.txt\" for each page or section created.\n",
      "- FINISH\n",
      "Expected result: A file named \"pages.txt\" containing the smaller pages or sections of the long text or code snippet.\n",
      "\n",
      "Task 5: Process each page or section individually\n",
      "- READ_FILE \"pages.txt\"\n",
      "- Instruct the user to process each page or section individually, using the summary from \"summary.txt\" to maintain context between pages.\n",
      "- CREATE_FILE \"processed_pages.txt\"\n",
      "- APPEND_TO_FILE \"processed_pages.txt\" for each processed page or section.\n",
      "- FINISH\n",
      "Expected result: A file named \"processed_pages.txt\" containing the processed pages or sections of the long text or code snippet.\n",
      "\n",
      "Task 6: Combine processed chunks and pages\n",
      "- CREATE_FILE \"combined_results.txt\"\n",
      "- READ_FILE \"processed_chunks.txt\"\n",
      "- APPEND_TO_FILE \"combined_results.txt\" with the contents of \"processed_chunks.txt\"\n",
      "- READ_FILE \"processed_pages.txt\"\n",
      "- APPEND_TO_FILE \"combined_results.txt\" with the contents of \"processed_pages.txt\"\n",
      "- FINISH\n",
      "Expected result: A file named \"combined_results.txt\" containing the combined results of the processed chunks and pages.\n",
      "Splitted Tasks as JSON [\n",
      "  \"Task 1: Create a summary file\\n- CREATE_FILE \\\"summary.txt\\\"\\n- Instruct the user to write a concise summary of the long text or code snippet, capturing the essential information needed to understand the context.\\n- FINISH\\nExpected result: A file named \\\"summary.txt\\\" containing a summary of the long text or code snippet.\",\n",
      "\n",
      "  \"Task 2: Create a file with smaller chunks\\n- CREATE_FILE \\\"chunks.txt\\\"\\n- Instruct the user to divide the long text or code snippet into smaller, manageable chunks that fit within the model's token limit.\\n- APPEND_TO_FILE \\\"chunks.txt\\\" for each chunk created.\\n- FINISH\\nExpected result: A file named \\\"chunks.txt\\\" containing the smaller chunks of the long text or code snippet.\",\n",
      "\n",
      "  \"Task 3: Process chunks and store results\\n- READ_FILE \\\"chunks.txt\\\"\\n- CREATE_FILE \\\"processed_chunks.txt\\\"\\n- Instruct the user to process each chunk separately with the language model, ensuring that the context is maintained across chunks.\\n- APPEND_TO_FILE \\\"processed_chunks.txt\\\" for each processed chunk.\\n- FINISH\\nExpected result: A file named \\\"processed_chunks.txt\\\" containing the processed chunks of the long text or code snippet.\",\n",
      "\n",
      "  \"Task 4: Create a file with the paging structure\\n- CREATE_FILE \\\"paging_structure.txt\\\"\\n- Instruct the user to write the paging structure for dividing the content into smaller pages or sections.\\n- FINISH\\nExpected result: A file named \\\"paging_structure.txt\\\" containing the paging structure for dividing the content into smaller pages or sections.\",\n",
      "\n",
      "  \"Task 5: Create a file with smaller pages or sections\\n- READ_FILE \\\"paging_structure.txt\\\"\\n- CREATE_FILE \\\"pages.txt\\\"\\n- Instruct the user to divide the content of the long text or code snippet into smaller pages or sections based on the paging structure.\\n- APPEND_TO_FILE \\\"pages.txt\\\" for each page or section created.\\n- FINISH\\nExpected result: A file named \\\"pages.txt\\\" containing the smaller pages or sections of the long text or code snippet.\",\n",
      "\n",
      "  \"Task 6: Process pages or sections and store results\\n- READ_FILE \\\"pages.txt\\\"\\n- READ_FILE \\\"summary.txt\\\"\\n- CREATE_FILE \\\"processed_pages.txt\\\"\\n- Instruct the user to process each page or section individually, using the summary from \\\"summary.txt\\\" to maintain context between pages.\\n- APPEND_TO_FILE \\\"processed_pages.txt\\\" for each processed page or section.\\n- FINISH\\nExpected result: A file named \\\"processed_pages.txt\\\" containing the processed pages or sections of the long text or code snippet.\",\n",
      "\n",
      "  \"Task 7: Combine processed chunks and pages\\n- CREATE_FILE \\\"combined_results.txt\\\"\\n- READ_FILE \\\"processed_chunks.txt\\\"\\n- READ_FILE \\\"processed_pages.txt\\\"\\n- Instruct the user to combine the contents of \\\"processed_chunks.txt\\\" and \\\"processed_pages.txt\\\" in a meaningful way.\\n- APPEND_TO_FILE \\\"combined_results.txt\\\" with the combined contents.\\n- FINISH\\nExpected result: A file named \\\"combined_results.txt\\\" containing the combined results of the processed chunks and pages.\"\n",
      "]\n",
      "Unsolved[Create a file summary.txt containing a summary of what FinalAGI.py does, by iterating over all the pages of FinalAGI.py.]>>not solved\n",
      "  Unsolved[Task 7: Combine processed chunks and pages\n",
      "- CREATE_FILE \"combined_results.txt\"\n",
      "- READ_FILE \"processed_chunks.txt\"\n",
      "- READ_FILE \"processed_pages.txt\"\n",
      "- Instruct the user to combine the contents of \"processed_chunks.txt\" and \"processed_pages.txt\" in a meaningful way.\n",
      "- APPEND_TO_FILE \"combined_results.txt\" with the combined contents.\n",
      "- FINISH\n",
      "Expected result: A file named \"combined_results.txt\" containing the combined results of the processed chunks and pages.]>>not solved\n",
      "    Unsolved[Task 6: Process pages or sections and store results\n",
      "- READ_FILE \"pages.txt\"\n",
      "- READ_FILE \"summary.txt\"\n",
      "- CREATE_FILE \"processed_pages.txt\"\n",
      "- Instruct the user to process each page or section individually, using the summary from \"summary.txt\" to maintain context between pages.\n",
      "- APPEND_TO_FILE \"processed_pages.txt\" for each processed page or section.\n",
      "- FINISH\n",
      "Expected result: A file named \"processed_pages.txt\" containing the processed pages or sections of the long text or code snippet.]>>not solved\n",
      "      Unsolved[Task 5: Create a file with smaller pages or sections\n",
      "- READ_FILE \"paging_structure.txt\"\n",
      "- CREATE_FILE \"pages.txt\"\n",
      "- Instruct the user to divide the content of the long text or code snippet into smaller pages or sections based on the paging structure.\n",
      "- APPEND_TO_FILE \"pages.txt\" for each page or section created.\n",
      "- FINISH\n",
      "Expected result: A file named \"pages.txt\" containing the smaller pages or sections of the long text or code snippet.]>>not solved\n",
      "        Unsolved[Task 4: Create a file with the paging structure\n",
      "- CREATE_FILE \"paging_structure.txt\"\n",
      "- Instruct the user to write the paging structure for dividing the content into smaller pages or sections.\n",
      "- FINISH\n",
      "Expected result: A file named \"paging_structure.txt\" containing the paging structure for dividing the content into smaller pages or sections.]>>not solved\n",
      "          Unsolved[Task 3: Process chunks and store results\n",
      "- READ_FILE \"chunks.txt\"\n",
      "- CREATE_FILE \"processed_chunks.txt\"\n",
      "- Instruct the user to process each chunk separately with the language model, ensuring that the context is maintained across chunks.\n",
      "- APPEND_TO_FILE \"processed_chunks.txt\" for each processed chunk.\n",
      "- FINISH\n",
      "Expected result: A file named \"processed_chunks.txt\" containing the processed chunks of the long text or code snippet.]>>not solved\n",
      "            Unsolved[Task 2: Create a file with smaller chunks\n",
      "- CREATE_FILE \"chunks.txt\"\n",
      "- Instruct the user to divide the long text or code snippet into smaller, manageable chunks that fit within the model's token limit.\n",
      "- APPEND_TO_FILE \"chunks.txt\" for each chunk created.\n",
      "- FINISH\n",
      "Expected result: A file named \"chunks.txt\" containing the smaller chunks of the long text or code snippet.]>>not solved\n",
      "              Unsolved[Task 1: Create a summary file\n",
      "- CREATE_FILE \"summary.txt\"\n",
      "- Instruct the user to write a concise summary of the long text or code snippet, capturing the essential information needed to understand the context.\n",
      "- FINISH\n",
      "Expected result: A file named \"summary.txt\" containing a summary of the long text or code snippet.]>>not solved\n",
      "Splitting Done\n",
      "0:Create a file summary.txt containing a summary of what FinalAGI.py does, by iterating over all the pages of FinalAGI.py.\n",
      "1:Task 7: Combine processed chunks and pages\n",
      "- CREATE_FILE \"combined_results.txt\"\n",
      "- READ_FILE \"processed_chunks.txt\"\n",
      "- READ_FILE \"processed_pages.txt\"\n",
      "- Instruct the user to combine the contents of \"processed_chunks.txt\" and \"processed_pages.txt\" in a meaningful way.\n",
      "- APPEND_TO_FILE \"combined_results.txt\" with the combined contents.\n",
      "- FINISH\n",
      "Expected result: A file named \"combined_results.txt\" containing the combined results of the processed chunks and pages.\n",
      "2:Task 6: Process pages or sections and store results\n",
      "- READ_FILE \"pages.txt\"\n",
      "- READ_FILE \"summary.txt\"\n",
      "- CREATE_FILE \"processed_pages.txt\"\n",
      "- Instruct the user to process each page or section individually, using the summary from \"summary.txt\" to maintain context between pages.\n",
      "- APPEND_TO_FILE \"processed_pages.txt\" for each processed page or section.\n",
      "- FINISH\n",
      "Expected result: A file named \"processed_pages.txt\" containing the processed pages or sections of the long text or code snippet.\n",
      "3:Task 5: Create a file with smaller pages or sections\n",
      "- READ_FILE \"paging_structure.txt\"\n",
      "- CREATE_FILE \"pages.txt\"\n",
      "- Instruct the user to divide the content of the long text or code snippet into smaller pages or sections based on the paging structure.\n",
      "- APPEND_TO_FILE \"pages.txt\" for each page or section created.\n",
      "- FINISH\n",
      "Expected result: A file named \"pages.txt\" containing the smaller pages or sections of the long text or code snippet.\n",
      "4:Task 4: Create a file with the paging structure\n",
      "- CREATE_FILE \"paging_structure.txt\"\n",
      "- Instruct the user to write the paging structure for dividing the content into smaller pages or sections.\n",
      "- FINISH\n",
      "Expected result: A file named \"paging_structure.txt\" containing the paging structure for dividing the content into smaller pages or sections.\n",
      "5:Task 3: Process chunks and store results\n",
      "- READ_FILE \"chunks.txt\"\n",
      "- CREATE_FILE \"processed_chunks.txt\"\n",
      "- Instruct the user to process each chunk separately with the language model, ensuring that the context is maintained across chunks.\n",
      "- APPEND_TO_FILE \"processed_chunks.txt\" for each processed chunk.\n",
      "- FINISH\n",
      "Expected result: A file named \"processed_chunks.txt\" containing the processed chunks of the long text or code snippet.\n",
      "6:Task 2: Create a file with smaller chunks\n",
      "- CREATE_FILE \"chunks.txt\"\n",
      "- Instruct the user to divide the long text or code snippet into smaller, manageable chunks that fit within the model's token limit.\n",
      "- APPEND_TO_FILE \"chunks.txt\" for each chunk created.\n",
      "- FINISH\n",
      "Expected result: A file named \"chunks.txt\" containing the smaller chunks of the long text or code snippet.\n",
      "7:Task 1: Create a summary file\n",
      "- CREATE_FILE \"summary.txt\"\n",
      "- Instruct the user to write a concise summary of the long text or code snippet, capturing the essential information needed to understand the context.\n",
      "- FINISH\n",
      "Expected result: A file named \"summary.txt\" containing a summary of the long text or code snippet.\n",
      "Unsolved[Create a file summary.txt containing a summary of what FinalAGI.py does, by iterating over all the pages of FinalAGI.py.]>>not solved\n",
      "  Unsolved[Task 7: Combine processed chunks and pages\n",
      "- CREATE_FILE \"combined_results.txt\"\n",
      "- READ_FILE \"processed_chunks.txt\"\n",
      "- READ_FILE \"processed_pages.txt\"\n",
      "- Instruct the user to combine the contents of \"processed_chunks.txt\" and \"processed_pages.txt\" in a meaningful way.\n",
      "- APPEND_TO_FILE \"combined_results.txt\" with the combined contents.\n",
      "- FINISH\n",
      "Expected result: A file named \"combined_results.txt\" containing the combined results of the processed chunks and pages.]>>not solved\n",
      "    Unsolved[Task 6: Process pages or sections and store results\n",
      "- READ_FILE \"pages.txt\"\n",
      "- READ_FILE \"summary.txt\"\n",
      "- CREATE_FILE \"processed_pages.txt\"\n",
      "- Instruct the user to process each page or section individually, using the summary from \"summary.txt\" to maintain context between pages.\n",
      "- APPEND_TO_FILE \"processed_pages.txt\" for each processed page or section.\n",
      "- FINISH\n",
      "Expected result: A file named \"processed_pages.txt\" containing the processed pages or sections of the long text or code snippet.]>>not solved\n",
      "      Unsolved[Task 5: Create a file with smaller pages or sections\n",
      "- READ_FILE \"paging_structure.txt\"\n",
      "- CREATE_FILE \"pages.txt\"\n",
      "- Instruct the user to divide the content of the long text or code snippet into smaller pages or sections based on the paging structure.\n",
      "- APPEND_TO_FILE \"pages.txt\" for each page or section created.\n",
      "- FINISH\n",
      "Expected result: A file named \"pages.txt\" containing the smaller pages or sections of the long text or code snippet.]>>not solved\n",
      "        Unsolved[Task 4: Create a file with the paging structure\n",
      "- CREATE_FILE \"paging_structure.txt\"\n",
      "- Instruct the user to write the paging structure for dividing the content into smaller pages or sections.\n",
      "- FINISH\n",
      "Expected result: A file named \"paging_structure.txt\" containing the paging structure for dividing the content into smaller pages or sections.]>>not solved\n",
      "          Unsolved[Task 3: Process chunks and store results\n",
      "- READ_FILE \"chunks.txt\"\n",
      "- CREATE_FILE \"processed_chunks.txt\"\n",
      "- Instruct the user to process each chunk separately with the language model, ensuring that the context is maintained across chunks.\n",
      "- APPEND_TO_FILE \"processed_chunks.txt\" for each processed chunk.\n",
      "- FINISH\n",
      "Expected result: A file named \"processed_chunks.txt\" containing the processed chunks of the long text or code snippet.]>>not solved\n",
      "            Unsolved[Task 2: Create a file with smaller chunks\n",
      "- CREATE_FILE \"chunks.txt\"\n",
      "- Instruct the user to divide the long text or code snippet into smaller, manageable chunks that fit within the model's token limit.\n",
      "- APPEND_TO_FILE \"chunks.txt\" for each chunk created.\n",
      "- FINISH\n",
      "Expected result: A file named \"chunks.txt\" containing the smaller chunks of the long text or code snippet.]>>not solved\n",
      "              Solved[Task 1: Create a summary file\n",
      "- CREATE_FILE \"summary.txt\"\n",
      "- Instruct the user to write a concise summary of the long text or code snippet, capturing the essential information needed to understand the context.\n",
      "- FINISH\n",
      "Expected result: A file named \"summary.txt\" containing a summary of the long text or code snippet.]>>Created \"summary.txt\" containing a summary of the FinalAGI.py script, which uses the Langchain library to interact with OpenAI's GPT-3 model and manage virtual files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=380).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=380).\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import deque\n",
    "from typing import Dict, Iterable, List, Set, Optional, Any\n",
    "\n",
    "from langchain import LLMChain, OpenAI, PromptTemplate\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.llms import BaseLLM\n",
    "from langchain.vectorstores.base import VectorStore\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.chains.base import Chain\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "import os\n",
    "from langchain.chat_models import PromptLayerChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "import sqlite3\n",
    "\n",
    "class VirtualFile:\n",
    "    def __init__(self, content, page_size):\n",
    "        self.pages = [content[i:i+page_size] for i in range(0, len(content), page_size)]\n",
    "\n",
    "\n",
    "class VirtualFileSystem:\n",
    "    def __init__(self, page_size=2000):\n",
    "        self.virtual_files = {}\n",
    "        self.page_size = page_size\n",
    "\n",
    "    def dump_all(self, dump_directory=\"dump\"):\n",
    "        if not os.path.exists(dump_directory):\n",
    "            os.makedirs(dump_directory)\n",
    "\n",
    "        for file_name in self.virtual_files:\n",
    "            file_path = os.path.join(dump_directory, file_name)\n",
    "            with open(file_path, 'w') as f:\n",
    "                for page in self.virtual_files[file_name].pages:\n",
    "                    f.write(page)\n",
    "        return f\"All virtual files saved to '{dump_directory}' directory.\"\n",
    "\n",
    "\n",
    "    def create_file(self, file_name, content):\n",
    "        if file_name in self.virtual_files:\n",
    "            return f\"Error: File '{file_name}' already exists.\"\n",
    "        self.virtual_files[file_name] = VirtualFile(content, self.page_size)\n",
    "        return f\"File '{file_name}' created.\"\n",
    "\n",
    "    def read_file(self, file_name, page, include_surrounding=False, surrounding_chars=100):\n",
    "        if file_name not in self.virtual_files:\n",
    "            return f\"Error: No such file: '{file_name}'\"\n",
    "        if page >= len(self.virtual_files[file_name].pages):\n",
    "            return f\"Error: Invalid page: {page}\"\n",
    "        \n",
    "        content = self.virtual_files[file_name].pages[page]\n",
    "\n",
    "        if include_surrounding:\n",
    "            prev_page = self.virtual_files[file_name].pages[page - 1] if page > 0 else \"\"\n",
    "            next_page = self.virtual_files[file_name].pages[page + 1] if page < len(self.virtual_files[file_name].pages) - 1 else \"\"\n",
    "\n",
    "            prev_content = prev_page[-surrounding_chars:] if prev_page else \"\"\n",
    "            next_content = next_page[:surrounding_chars] if next_page else \"\"\n",
    "\n",
    "            content = f\"{prev_content}{content}{next_content}\"\n",
    "\n",
    "        return content\n",
    "\n",
    "    def update_file(self, file_name, page, new_content):\n",
    "        if file_name not in self.virtual_files:\n",
    "            return f\"Error: No such file: '{file_name}'\"\n",
    "        if page >= len(self.virtual_files[file_name].pages):\n",
    "            return f\"Error: Invalid page: {page}\"\n",
    "        self.virtual_files[file_name].pages[page] = new_content\n",
    "        self.reorganize_pages(file_name)\n",
    "        return f\"File '{file_name}' updated at page {page}.\"\n",
    "\n",
    "    def save_to_disk(self, file_name, file_path=None):\n",
    "        if file_name not in self.virtual_files:\n",
    "            return f\"Error: No such file: '{file_name}'\"\n",
    "        if not file_path:\n",
    "            file_path = file_name\n",
    "        with open(file_path, 'w') as f:\n",
    "            for page in self.virtual_files[file_name].pages:\n",
    "                f.write(page)\n",
    "        return f\"File '{file_name}' saved to disk at '{file_path}'.\"\n",
    "    \n",
    "    def load_from_disk(self, file_path, file_name=None):\n",
    "        if not os.path.exists(file_path):\n",
    "            return f\"Error: No such file on disk: '{file_path}'\"\n",
    "        if not file_name:\n",
    "            file_name = os.path.basename(file_path)\n",
    "        if file_name in self.virtual_files:\n",
    "            return f\"Error: File already exists in VFS: '{file_name}'\"\n",
    "        with open(file_path, 'r') as f:\n",
    "            content = f.read()\n",
    "        self.virtual_files[file_name] = VirtualFile(content, self.page_size)\n",
    "        return f\"File '{file_path}' loaded from disk into VFS as '{file_name}'.\"\n",
    "\n",
    "    def list_files(self):\n",
    "        output = \"Current available Files:\\n\"\n",
    "        for file_name in self.virtual_files.keys():\n",
    "            file = self.virtual_files[file_name]\n",
    "            output += f\"File '{file_name}' has {len(file.pages)} pages and a total size of {sum(len(page) for page in file.pages)} characters.\\n\"\n",
    "        return output+\"\\n\"\n",
    "\n",
    "    def delete_file(self, file_name):\n",
    "        if file_name not in self.virtual_files:\n",
    "            return f\"Error: No such file: '{file_name}'\"\n",
    "        del self.virtual_files[file_name]\n",
    "        return f\"File '{file_name}' deleted.\"\n",
    "\n",
    "    def append_to_file(self, file_name, content):\n",
    "        if file_name not in self.virtual_files:\n",
    "            return f\"Error: No such file: '{file_name}'\"\n",
    "        self.virtual_files[file_name].pages.extend(\n",
    "            [content[i:i+self.page_size] for i in range(0, len(content), self.page_size)]\n",
    "        )\n",
    "        return f\"Content appended to file '{file_name}'.\"\n",
    "\n",
    "    def rename_file(self, old_name, new_name):\n",
    "        if old_name not in self.virtual_files:\n",
    "            return f\"Error: No such file: '{old_name}'\"\n",
    "        if new_name in self.virtual_files:\n",
    "            return f\"Error: File already exists: '{new_name}'\"\n",
    "        self.virtual_files[new_name] = self.virtual_files.pop(old_name)\n",
    "        return f\"File '{old_name}' renamed to '{new_name}'.\"\n",
    "\n",
    "    def get_file_info(self, file_name):\n",
    "        if file_name not in self.virtual_files:\n",
    "            return f\"Error: No such file: '{file_name}'\"\n",
    "        file = self.virtual_files[file_name]\n",
    "        return f\"File '{file_name}' has {len(file.pages)} pages and a total size of {sum(len(page) for page in file.pages)} characters.\"\n",
    "\n",
    "    def reorganize_pages(self, file_name):\n",
    "        if file_name not in self.virtual_files:\n",
    "            return f\"Error: No such file: '{file_name}'\"\n",
    "        file = self.virtual_files[file_name]\n",
    "        content = \"\".join(file.pages)\n",
    "        file.pages = [content[i:i+self.page_size] for i in range(0, len(content), self.page_size)]\n",
    "        return f\"Pages reorganized for file '{file_name}'.\"\n",
    "    \n",
    "\n",
    "    def execute_command(self, command):\n",
    "        command_parts = command.split()\n",
    "        if not command_parts:\n",
    "            return \"Error: Empty command.\"\n",
    "\n",
    "        command_name = command_parts[0].upper()\n",
    "        args = command_parts[1:]\n",
    "\n",
    "        if command_name == \"CREATE_FILE\":\n",
    "            if len(args) < 2:\n",
    "                return \"Error: CREATE_FILE requires at least 2 arguments.\"\n",
    "            return self.create_file(args[0], \" \".join(args[1:]))\n",
    "\n",
    "        elif command_name == \"READ_FILE\":\n",
    "            if len(args) < 2:\n",
    "                return \"Error: READ_FILE requires at least 2 arguments.\"\n",
    "            include_surrounding = \"INCLUDE_SURROUNDING\" in args\n",
    "            if include_surrounding:\n",
    "                args.remove(\"INCLUDE_SURROUNDING\")\n",
    "            surrounding_chars_index = next((i for i, x in enumerate(args) if x == \"SURROUNDING_CHARS\"), None)\n",
    "            if surrounding_chars_index is not None and surrounding_chars_index + 1 < len(args):\n",
    "                surrounding_chars = int(args[surrounding_chars_index + 1])\n",
    "                args = args[:surrounding_chars_index] + args[surrounding_chars_index + 2:]\n",
    "            else:\n",
    "                surrounding_chars = 100\n",
    "            return self.read_file(args[0], int(args[1]), include_surrounding, surrounding_chars)\n",
    "\n",
    "        elif command_name == \"UPDATE_FILE\":\n",
    "            if len(args) < 3:\n",
    "                return \"Error: UPDATE_FILE requires at least 3 arguments.\"\n",
    "            return self.update_file(args[0], int(args[1]), \" \".join(args[2:]))\n",
    "\n",
    "        elif command_name == \"SAVE_TO_DISK\":\n",
    "            if len(args) < 1:\n",
    "                return \"Error: SAVE_TO_DISK requires at least 1 argument.\"\n",
    "            return self.save_to_disk(args[0], args[1] if len(args) > 1 else None)\n",
    "\n",
    "        elif command_name == \"LIST_FILES\":\n",
    "            return self.list_files()\n",
    "\n",
    "        elif command_name == \"DELETE_FILE\":\n",
    "            if len(args) < 1:\n",
    "                return \"Error: DELETE_FILE requires at least 1 argument.\"\n",
    "            return self.delete_file(args[0])\n",
    "\n",
    "        elif command_name == \"APPEND_TO_FILE\":\n",
    "            if len(args) < 2:\n",
    "                return \"Error: APPEND_TO_FILE requires at least 2 arguments.\"\n",
    "            return self.append_to_file(args[0], \" \".join(args[1:]))\n",
    "\n",
    "        elif command_name == \"RENAME_FILE\":\n",
    "            if len(args) < 2:\n",
    "                return \"Error: RENAME_FILE requires at least 2 arguments.\"\n",
    "            return self.rename_file(args[0], args[1])\n",
    "\n",
    "        elif command_name == \"GET_FILE_INFO\":\n",
    "            if len(args) < 1:\n",
    "                return \"Error: GET_FILE_INFO requires at least 1 argument.\"\n",
    "            return self.get_file_info(args[0])\n",
    "\n",
    "        elif command_name == \"REORGANIZE_PAGES\":\n",
    "            if len(args) < 1:\n",
    "                return \"Error: REORGANIZE_PAGES requires at least 1 argument.\"\n",
    "            return self.reorganize_pages(args[0])\n",
    "\n",
    "        elif command_name == \"DUMP_ALL\":\n",
    "            return self.dump_all(args[0] if args else None)\n",
    "\n",
    "        else:\n",
    "            return f\"Error: Unknown command '{command_name}'.\"\n",
    "\n",
    "class Task:\n",
    "    def __init__(self, TaskDescription: str = \"dscr\", resultDescription:str = \"not solved\", Solved:bool = False):\n",
    "        self.Description:str = TaskDescription\n",
    "        self.ResultDescription:str = resultDescription\n",
    "        self.Solved:bool = Solved\n",
    "        self.ChildTasks:List[Task] = []\n",
    "        self.ParentTasks:List[Task] = []\n",
    "        \n",
    "    def __str__(self)->str:\n",
    "        if(self.Solved):\n",
    "            return \"Solved[\"+self.Description+\"]>>\"+self.ResultDescription\n",
    "        else:\n",
    "            return \"Unsolved[\"+self.Description+\"]>>\"+self.ResultDescription\n",
    "        \n",
    "    def get_recusively_all_subtasks(self, visited: Set['Task'] = None) -> List['Task']:\n",
    "        if visited is None:\n",
    "            visited = set()\n",
    "        if self in visited:\n",
    "            return []\n",
    "        all_tasks = []  # Add the current task to the list\n",
    "        for subtask in self.ChildTasks:\n",
    "            all_tasks.append(subtask)\n",
    "            all_tasks +=subtask.get_recusively_all_subtasks(visited)  # Recursively get all subtasks of the subtask\n",
    "        return all_tasks\n",
    "\n",
    "    def display(self, indent_level: int = 0) -> None:\n",
    "        indent = '  ' * indent_level  # Modify the number of spaces in the quotes to adjust indentation\n",
    "        if(self.Solved):\n",
    "            print(indent+\"Solved[\"+self.Description+\"]>>\"+self.ResultDescription.replace(\"\\n\",\"\"))\n",
    "        else:\n",
    "            print(indent+\"Unsolved[\"+self.Description+\"]>>\"+self.ResultDescription)\n",
    "        for subtask in self.ChildTasks:\n",
    "            subtask.display(indent_level + 1)\n",
    "    \n",
    "    def collect_parent_descriptions(self, visited: Set['Task'] = None) -> List[str]:\n",
    "        if visited is None:\n",
    "            visited = set()\n",
    "        if self in visited:\n",
    "            return []\n",
    "        visited.add(self)\n",
    "        descriptions = []\n",
    "        for parent_task in self.ParentTasks:\n",
    "            descriptions.append(parent_task.Description)\n",
    "            descriptions.extend(parent_task.collect_parent_descriptions(visited))\n",
    "        return descriptions\n",
    "    \n",
    "\n",
    "    def is_atomic(self)->bool:\n",
    "        return len(self.ChildTasks) == 0\n",
    "    \n",
    "    def ready_to_solve(self)->bool:\n",
    "            return all(map(lambda x: x.Solved, self.ChildTasks)) #all([]) is True\n",
    "        \n",
    "    def add_child(self, child: 'Task') -> None:\n",
    "        if child in self.ChildTasks:\n",
    "            return\n",
    "        self.ChildTasks.append(child)\n",
    "        child.ParentTasks.append(self)\n",
    "\n",
    "    def add_childs(self, childs: List['Task']) -> None:\n",
    "        for child in childs:\n",
    "            self.add_child(child)\n",
    "\n",
    "class Taskpool(List[Task]):    \n",
    "    def __str__(self)->str:\n",
    "        return \",\".join(map(str, self.tasks))\n",
    "    \n",
    "    def CreateContextSummery(self)->str:\n",
    "        return f\"/n\".join(map(lambda x: x.TaskDescription, self.tasks))\n",
    "    \n",
    "    def get_task_ready_to_solve(self)->Task:\n",
    "        for task in self:\n",
    "            if(task.Solved):\n",
    "                continue\n",
    "            if(task.ready_to_solve()):\n",
    "                return task\n",
    "        raise Exception(\"No task ready to solve\")\n",
    "    \n",
    "    def get_nummerated_task_List(self) -> str:\n",
    "        formatted_tasks = []\n",
    "        for index, task in enumerate(self):\n",
    "            formatted_tasks.append(f\"{index}:{task.Description}\")\n",
    "        return \"\\n\".join(formatted_tasks)\n",
    "    \n",
    "class CachedChat:\n",
    "    def __init__(self):\n",
    "        self.conn = None\n",
    "        self.chat = PromptLayerChatOpenAI(model_name=\"gpt-4\",temperature=0.2, request_timeout=380)\n",
    "        # self.chat = PromptLayerChatOpenAI(temperature=0.2, request_timeout=180)\n",
    "\n",
    "    def __call__(self, messages, stop=None):\n",
    "\n",
    "        return AIMessage(\n",
    "            content=self.run_chat(messages, stop)\n",
    "        )\n",
    "    \n",
    "    def establish_connection(self):\n",
    "        self.conn = sqlite3.connect('cache.db')\n",
    "\n",
    "    def check_cache(self, query):\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute(\"SELECT response FROM cache WHERE query=?\", (query,))\n",
    "        result = cursor.fetchone()\n",
    "        return result[0] if result else None\n",
    "\n",
    "    def store_cache(self, query, response):\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute(\"INSERT INTO cache(query, response) VALUES (?, ?)\", (query, response))\n",
    "        self.conn.commit()\n",
    "\n",
    "    def create_cache_table(self):\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute('''CREATE TABLE IF NOT EXISTS cache\n",
    "                        (query TEXT PRIMARY KEY, response TEXT)''')\n",
    "        self.conn.commit()\n",
    "\n",
    "    def run_chat(self, messages, stop) -> str:\n",
    "        query = str((messages, stop))\n",
    "        self.establish_connection()\n",
    "        self.create_cache_table()\n",
    "        cached_response = self.check_cache(query)\n",
    "\n",
    "        if cached_response:\n",
    "            return cached_response\n",
    "\n",
    "        chat_response = self.chat(messages, stop)\n",
    "        self.store_cache(query, chat_response.content)\n",
    "\n",
    "        return chat_response.content\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, Objectiv: str):\n",
    "        self.Objectiv:Task = Task(Objectiv)\n",
    "        self.Taskpool = Taskpool()\n",
    "        self.Taskpool.append(self.Objectiv)\n",
    "        # self.chat = ChatOpenAI(temperature=0)\n",
    "        #self.chat = PromptLayerChatOpenAI(temperature=0.2)\n",
    "        self.chat=CachedChat()\n",
    "\n",
    "        self.VirtualFileSystem=VirtualFileSystem(page_size=2000)\n",
    "\n",
    "        self.command_documentation = \"\"\"Command Documentation\n",
    "CALL CREATE_FILE <file_name>\n",
    "<content>\n",
    "ENDCALL\n",
    "- Creates a new file with the given name and content.\n",
    "\n",
    "CALL READ_FILE <file_name> <page> [INCLUDE_SURROUNDING] [SURROUNDING_CHARS <num_chars>]\n",
    "ENDCALL\n",
    "- Reads the specified page of the file. Optionally, include surrounding content from adjacent pages with the specified number of characters.\n",
    "\n",
    "CALL UPDATE_FILE <file_name> <page>\n",
    "<new_content>\n",
    "ENDCALL\n",
    "- Updates the specified page of the file with the new content.\n",
    "\n",
    "CALL LIST_FILES\n",
    "ENDCALL\n",
    "- Lists all the files in the FS.\n",
    "\n",
    "CALL DELETE_FILE <file_name>\n",
    "ENDCALL\n",
    "- Deletes the specified file from the FS.\n",
    "\n",
    "CALL APPEND_TO_FILE <file_name>\n",
    "<content>\n",
    "ENDCALL\n",
    "- Appends the given content to the specified file.\n",
    "\n",
    "CALL RENAME_FILE <old_name> <new_name>\n",
    "ENDCALL\n",
    "- Renames the specified file to the new name.\n",
    "\n",
    "CALL GET_FILE_INFO <file_name>\n",
    "ENDCALL\n",
    "- Retrieves information about the specified file, such as the number of pages and total size.\n",
    "\n",
    "CALL FINISH\n",
    "<Summary of files created and their contents as hints for Parent Tasks>\n",
    "ENDCALL\n",
    "- Returns the summary of files created and their contents as hints for Parent Tasks. This ends the solving of the task. Call this when you are done.\n",
    "\n",
    "Example:\n",
    "CALL CREATE_FILE test.txt\n",
    "This is the content of the file\n",
    "ENDCALL\n",
    "\n",
    "The first Page in a File is Page number 0.\n",
    "\n",
    "Only ever create one command. You will be provided the results of an command and than are prompted again to create a new Command.\n",
    "Its not your task to simulate the command execution! Dont create Returnmessages for Commands.\n",
    "\n",
    "Suggested Workflow:\n",
    "Get current list of files.\n",
    "Read Files that could be usefull, if any\n",
    "Create new File with your solution\n",
    "finish with FINISH command\n",
    "\n",
    "\"\"\"\n",
    "    def extract_json(self,text):\n",
    "        # Find a JSON object or array pattern in the text\n",
    "        pattern = r'({.*?}|\\[.*?\\])'\n",
    "        match = re.search(pattern, text, re.DOTALL)\n",
    "\n",
    "        # Check if a match is found\n",
    "        if match:\n",
    "            json_str = match.group()\n",
    "            try:\n",
    "                # Parse the JSON string into a Python object\n",
    "                json_obj = json.loads(json_str)\n",
    "                return json_obj\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"Invalid JSON detected.\")\n",
    "                return None\n",
    "        else:\n",
    "            print(\"No JSON found in the text.\")\n",
    "            return None\n",
    "\n",
    "    def should_be_split(self, task: Task) -> bool:\n",
    "        #test if task is atomic if no throw exception\n",
    "        if len(task.ChildTasks) != 0:\n",
    "            raise ValueError(\"Task is not atomic\")\n",
    "        \n",
    "        print(\"Now in should_be_slpit:\" + task.Description)\n",
    "\n",
    "        \n",
    "       \n",
    "\n",
    "        messages = []\n",
    "        messages.append(SystemMessage(content=\n",
    "                                      \"You are an AI that determines if a task could be split into multiple simpler tasks. Return 'True' if it could be split and 'False' otherwise. Dont be chaty.\"))\n",
    "        \n",
    "        messages.append(HumanMessage(content=\n",
    "                                     f\"Return 'True' if it could be split and 'False' otherwise. Task description: {task.Description}\"))\n",
    "        \n",
    "        max_retries = 3\n",
    "        for _ in range(max_retries):\n",
    "            response = self.chat(messages).content\n",
    "            clean_response = response.strip().lower()\n",
    "\n",
    "            if 'true' in clean_response:\n",
    "                return True\n",
    "            elif 'false' in clean_response:\n",
    "                return False\n",
    "            else:\n",
    "                messages.append(HumanMessage(content=\"Please provide a valid response with either 'True' or 'False'.\"))\n",
    "        raise ValueError(\"AI response does not contain a valid boolean string after multiple retries: \" + response)\n",
    "\n",
    "    def SplitTask(self, task: Task, verbose: bool = True) -> List[Task]:\n",
    "        print(\"Now in SplitTask:\" + task.Description)\n",
    "\n",
    "        MainPromp=\"\"\"You are a Task splitting AI. Your objective is to analyze the main task provided and create smaller, independent, and manageable subtasks that contribute to its successful completion. Create a hierarchical view, prioritizing tasks that need to be completed first. Ensure that the subtasks do not expand the scope of the main task and follow it literally. In each subtask: - instruct the use of the FileSystem to store results\\n - return information about the created or modified files, along with a brief description of their contents. \\n\\n Clearly define the expected results and file names for each task to be produced. You can not create variables, only files.\\n\\n\"\"\"\n",
    "        messages = []\n",
    "\n",
    "        messages.append(SystemMessage(content=MainPromp))\n",
    "        messages.append(HumanMessage(content=f\"\"\"The limited context problem refers to the challenge faced by language models like ChatGPT when processing and generating text that exceeds their maximum token limit. For example, GPT-3 has a token limit of 4096 tokens. When dealing with long text or code snippets beyond this limit, the model may lose important context, leading to incorrect or incomplete analysis and generation. To overcome this limitation, you can use summarization, subtask creation, and paging of files.\n",
    "\n",
    "Please use the following strategies to address the limited context problem:\n",
    "\n",
    "1. Summarization: Create concise summaries of the long text or code snippets, capturing the essential information needed to understand the context. This will help the language model process the information more effectively.\n",
    "\n",
    "2. Subtask creation: Break down the long text or code snippet into smaller, manageable chunks that fit within the model's token limit. Process each chunk separately with the language model, ensuring that the context is maintained across chunks.\n",
    "\n",
    "3. Paging of files: When dealing with large files, divide the content into smaller pages or sections. Process each page or section individually, using summarization to maintain context between pages.\n",
    "\n",
    "By employing these strategies, you can effectively process long text or code snippets while preserving context and ensuring accurate analysis and generation. subtasks should be specific and detailed to ensure that they are easy to follow and produce the desired results. \n",
    "\n",
    "Allways instruct in each Task to call the FINISH command at the end of an Task.\n",
    "These are the available commands to the Tasksolver, keep this in mind when creating the subtasks.\n",
    "Do not number your tasks, but list them in the order they should be executed.\n",
    "\n",
    "The Tasksolver has the Following Commands available:\n",
    "CREATE_FILE READ_FILE UPDATE_FILE LIST_FILES DELETE_FILE APPEND_TO_FILE RENAME_FILE GET_FILE_INFO FINISH\n",
    "\n",
    "But do not mirco manage the Task Solving Agent by telling him what exact commands to use. He is able to figure out what to do by himself.\n",
    "\n",
    "{MainPromp}\n",
    "\"\"\"))\n",
    "\n",
    "        # Check if the task has any parent tasks\n",
    "        # parent_descriptions = task.collect_parent_descriptions()\n",
    "        # if parent_descriptions:\n",
    "        #     # Include the descriptions of the parent tasks in the task description\n",
    "        #     task_description = f\" Maintask: {task.Description}. The context of the parent tasks is: {', '.join(parent_descriptions)}\"\n",
    "        #     task_description += \"\\n Use this context to split the main task into smaller, independent, and more manageable subtasks.\"\n",
    "        #     messages.append(HumanMessage(content=task_description))\n",
    "        # else:\n",
    "        #     messages.append(HumanMessage(content=MainPromp+ \"\"\" Main task to split: \"\"\" + task.Description))\n",
    "\n",
    "        # Call OpenAI\n",
    "        generation = self.chat(messages)\n",
    "\n",
    "        print(\"Splitted Tasks before JSON packing: \" + generation.content)\n",
    "\n",
    "\n",
    "        #refinment step\n",
    "        messages.append(generation)\n",
    "        messages.append(HumanMessage(content=\"\"\"Have you followed ale the rules descriped to create this task list? How could better Subtasks look?\"\"\"))\n",
    "        generation = self.chat(messages)\n",
    "        messages.append(generation)\n",
    "\n",
    "        messages.append(HumanMessage(content=\"\"\"Now create a improved version of the task list.\"\"\"))\n",
    "        generation = self.chat(messages)\n",
    "        messages.append(generation)\n",
    "\n",
    "\n",
    "        # JSON Packaging step\n",
    "        messages.append(generation)\n",
    "        messages.append(HumanMessage(content=\"\"\"Now put this Tasks with their complete description in full detail in an element of a JSON Array like [\"Complete Description of first Task....\", \"Complete description of second Task....\"].\"\"\"))\n",
    "\n",
    "        # Call OpenAI\n",
    "        JSONSTR = self.chat(messages).content\n",
    "        print(\"Splitted Tasks as JSON \" + JSONSTR)\n",
    "\n",
    "        JSON = self.extract_json(JSONSTR)\n",
    "\n",
    "        Tasks = []\n",
    "\n",
    "        for subtaskstr in JSON:\n",
    "            Tasks.append(Task(TaskDescription=subtaskstr))\n",
    "\n",
    "        # Link the tasks as child tasks in a sequential manner, each task is a child task of the task before it\n",
    "        Tasks.reverse()  # Last Task depends on Tasks before it so he is the first with a child\n",
    "        task.add_child(Tasks[0])\n",
    "        for i in range(len(Tasks) - 1):\n",
    "            Tasks[i].add_child(Tasks[i + 1])\n",
    "\n",
    "        return Tasks\n",
    "    \n",
    "    def SolveAtomicTask(self, task: Task, max_retries: int = 5, verbose: bool = True) -> Task:\n",
    "\n",
    "        for i in range(max_retries):\n",
    "            finalMessages = []\n",
    "            finalMessages.append(SystemMessage(content=\"You are an AI designed to solve tasks by understanding the requirements, performing necessary file operations, and delivering the results. Stay focused on the task and adhere to the command documentation. Save the task results in files and provide a brief message indicating the created files and a concise description of their contents. Once completed, use the FINISH command.\"))\n",
    "\n",
    "            # Include Parent Tasks in the task description\n",
    "            parent_descriptions = task.collect_parent_descriptions()\n",
    "            parent_descriptions.reverse() # Reverse the order of the parent descriptions so that the 'nearest' parent is last to be included in the task description (most relevant to the current task)\n",
    "            if parent_descriptions:\n",
    "                task_description = f\"Current Task: {task.Description}. \\n\\n This Current Task will be used to solve the parent tasks. Keep that in mind while solving it, so its solved in a way, its usefull in the parent Tasks.\\n\\n Parent Tasks: {os.linesep.join(parent_descriptions)}\"\n",
    "\n",
    "                finalMessages.append(HumanMessage(content=task_description))\n",
    "\n",
    "            childs = task.get_recusively_all_subtasks()\n",
    "            if len(childs) > 0:\n",
    "                task_description = f\"Current Task: {task.Description}. The Current Task has Subtasks that are solved. The results and descriptions of the child tasks are:\\n\\n\" + '\\n\\n'.join(map(lambda t: (t.Description + \": \" + t.ResultDescription), childs))\n",
    "                task_description += \"\\n Use these results to solve the current task.\"\n",
    "                finalMessages.append(HumanMessage(content=task_description))\n",
    "\n",
    "            finalMessages.append(HumanMessage(content=self.command_documentation))\n",
    "\n",
    "            finalMessages.append(HumanMessage(content=self.VirtualFileSystem.list_files()))\n",
    "            finalMessages.append(HumanMessage(content=f\"Current Task: {task.Description}\"))\n",
    "\n",
    "            for i in range(10): #max 10 commands\n",
    "                \n",
    "\n",
    "\n",
    "                solutionMessage = self.chat(finalMessages, stop=[\"ENDCALL\"])\n",
    "                solutionMessage.content+=\"\\nENDCALL\" #get the stop back in\n",
    "                finalMessages.append(solutionMessage)\n",
    "\n",
    "\n",
    "                generation = solutionMessage.content.strip()\n",
    "                #todo extract command from generation\n",
    "                command_start = generation.find(\"CALL\")\n",
    "                command_end = generation.find(\"ENDCALL\")\n",
    "                if command_start != -1 and command_end != -1:\n",
    "                    command = generation[command_start + 4:command_end].strip()\n",
    "                    \n",
    "                    if \"CALL FINISH\" in generation:\n",
    "                        task.ResultDescription = command.replace(\"FINISH\\n\", \"\").strip()\n",
    "                        task.Solved = True\n",
    "                        return task\n",
    "\n",
    "                    command_result = self.VirtualFileSystem.execute_command(command)\n",
    "\n",
    "                    if command_result:\n",
    "                        finalMessages.append(HumanMessage(content=\"Command Return:\\n\"+command_result))\n",
    "\n",
    "                else:\n",
    "                    if verbose:\n",
    "                        print(f\"Retry {i + 1}: Command not found in generation.\")\n",
    "            if verbose:\n",
    "                print(f\"Failed to solve task after {max_retries} retries.\")\n",
    "        return task\n",
    "    \n",
    "    def run(self):\n",
    "        \n",
    "        #hardcoded passes of possible splitting\n",
    "        for i in range(1):\n",
    "            currentTasks = Taskpool()\n",
    "            currentTasks.extend(self.Taskpool) #create copy of Taskpool or else loop will be endless\n",
    "            for task in currentTasks:\n",
    "                if task.is_atomic():\n",
    "                    #if self.should_be_split(task):\n",
    "                    splitDone=True\n",
    "                    print(\"Splitting Task\")\n",
    "                    newTasks = self.SplitTask(task)\n",
    "                    self.Taskpool.extend(newTasks)\n",
    "                    self.Objectiv.display()\n",
    "                    print(\"Splitting Done\")\n",
    "        \n",
    "        print(self.Taskpool.get_nummerated_task_List())\n",
    "\n",
    "        while self.Objectiv.Solved==False:\n",
    "            currentTask=self.Taskpool.get_task_ready_to_solve()\n",
    "            #self.DummySolveAtomicTask(currentTask)\n",
    "            self.SolveAtomicTask(currentTask)\n",
    "            self.Objectiv.display()\n",
    "\n",
    "# agent = Agent(\"Create a file containing a workflow how to write a novel\")\n",
    "# agent = Agent(\"Write a single HTML file in wich with JavaScript two AIs play Connect 4 against each other. The User only watches, and restarts.\")\n",
    "\n",
    "agent=Agent(\"Create a file summary.txt containing a summary of what FinalAGI.py does, by iterating over all the pages of FinalAGI.py.\")\n",
    "agent.VirtualFileSystem.load_from_disk(\"FinalAGI.py\")\n",
    "agent.run()\n",
    "agent.VirtualFileSystem.dump_all()\n",
    "\n",
    "print(agent.Objectiv.ResultDescription)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
